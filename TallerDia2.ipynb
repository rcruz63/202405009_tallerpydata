{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0fb87cb-dc2f-46d2-8532-9d029c891f11",
   "metadata": {},
   "source": [
    "# Taller Dia 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2881dcc9-964e-49ff-8959-961c73a4f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade tu api key aquí\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"***REMOVED***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf0a478-6683-4597-ad9c-17f70438fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71dce30d-2c04-44b4-b0e7-16b11b30b1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todo está ok! :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cac1080-e233-42d1-acbc-9d2dd737d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el cliente para los siguientes ejercicios\n",
    "import openai\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e87939c4-89f8-418a-8e23-b68bea911e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_chatbot2(code:str): \n",
    "    response = client.chat.completions.create(max_tokens=3000, model=\"gpt-3.5-turbo-1106\", \n",
    "                                              messages=[ {\"role\": \"system\", \"content\": \"Traduce al inglés\"}, \n",
    "                                                        {\"role\": \"user\", \"content\": \"Hola\"}]) \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28524c63-12db-445f-9648-da5c36727431",
   "metadata": {},
   "outputs": [],
   "source": [
    "codigo=f\"\"\"\n",
    "def explica_codigo(codigo):\n",
    "\n",
    "    respuesta = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=codigo)\n",
    "    return respuesta\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = explain_chatbot2(codigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7591867d-2a34-4158-8659-759462779267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El código proporcionado es una función de Python llamada `explica_codigo` que toma un argumento `codigo`. La función utiliza un cliente (llamado `client`) para realizar una llamada a un servicio de completado de texto, posiblemente relacionado con OpenAI's GPT-3.5, para obtener una respuesta basada en el código proporcionado como entrada. La respuesta generada por el servicio se devuelve como resultado de la función.\n",
      "\n",
      "Es importante tener en cuenta que el cliente utilizado en el código (referenciado como `client`) debe haber sido previamente definido e inicializado con las credenciales adecuadas para acceder al servicio de completado de texto. Además, es probable que el código de conexión y autorización necesite estar presente antes de ejecutar esta función.\n",
      "\n",
      "En resumen, la función `explica_codigo` parece estar destinada a proporcionar una explicación o información adicional basada en el código proporcionado como entrada, utilizando algún tipo de servicio de inteligencia artificial para completar y elaborar respuestas.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce90f9a-c282-4897-b056-90d02ab92eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_chatbot(code:str):\n",
    "    \n",
    "    response = llm(max_tokens=3000, model=\"gpt-3.5-turbo-1106\",\n",
    "                                       messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"Explica el código de python que te da el usuario\"},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": f\"\"\"\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\"\"\"}\n",
    "])\n",
    "    assert response.choices[0].finish_reason == \"stop\", \"Faltan tokens, incrementa max_tokens o reduce el prompt\"\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f994b-0233-46ce-a870-45e012553675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import openai\n",
    "from functools import lru_cache\n",
    "\n",
    "_cache = {}\n",
    "\n",
    "def make_cache_key(kwargs):\n",
    "    def convert_to_hashable(value):\n",
    "        if isinstance(value, dict):\n",
    "            return frozenset((k, convert_to_hashable(v)) for k, v in sorted(value.items()))\n",
    "        elif isinstance(value, list):\n",
    "            return tuple(convert_to_hashable(v) for v in value)\n",
    "        elif isinstance(value, set):\n",
    "            return frozenset(convert_to_hashable(v) for v in value)\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    return frozenset((k, convert_to_hashable(v)) for k, v in sorted(kwargs.items()))\n",
    "\n",
    "def llm(**kwargs):\n",
    "    \n",
    "    key = make_cache_key(kwargs)\n",
    "    try:\n",
    "        return _cache[key]\n",
    "    except KeyError:\n",
    "        _cache[key] = result = client.chat.completions.create(**kwargs)\n",
    "        return result\n",
    "\n",
    "@lru_cache(3000)\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "\n",
    "    client = openai.OpenAI()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "def get_module_functions(module_name):\n",
    "    module = __import__(module_name)\n",
    "    functions = []\n",
    "\n",
    "    for name, obj in inspect.getmembers(module):\n",
    "        if inspect.isfunction(obj) and obj.__module__ == module_name:\n",
    "            ds = inspect.getdoc(obj)\n",
    "            function_info = {\n",
    "                \"name\": name,\n",
    "                \"docstring\": ds\n",
    "            }\n",
    "            if ds:\n",
    "                functions.append(function_info)\n",
    "\n",
    "    return functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
